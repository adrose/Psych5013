---
title: "Quiz 1"
author: "Adon Rosen"
date: 'Date: `r Sys.Date()`'
output: pdf_document
---
  
```{r, echo=F, message=F, warning=FALSE}
library(knitr)
opts_knit$set(root.dir='./')  #Don't combine this call with any other chunk -especially one that uses file paths.
```


<!-- Set the report-wide options, and point to the external code file. -->
```{r set_options, echo=F, warning=FALSE}
# cat("Working directory: ", getwd())
report_render_start_time <- Sys.time()
opts_chunk$set(
  results    = 'show',
  comment    = NA,
  tidy       = FALSE,
  fig.width  = 10,
  fig.height = 6,
  fig.path   = 'figure-png/'
)
# echoChunks <- FALSE
options(width=80) #So the output is 50% wider than the default.
read_chunk("./Quiz1.R") #This allows knitr to call chunks tagged in the underlying *.R file.
```


```{r load-packages, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```


# Question 1
A person’s muscle mass is expected to decrease with age.  To explore this relationship in women, a nutritionist randomly selected 4 women from each 10-year age group, beginning with age 40 and ending with age 79, resulting in a total sample size of n=16.  Some results follow, where X is the age, and Y is a measure of the muscle mass.  Assume a simple linear regression model is appropriate.  
For these data:

Mean(X) = 61.69  SD(X) = 14.67		
Mean(Y)  = 52.21  SD(Y) = 24.16

y-hat = 142.68  – 1.47 *X  

SE(Slope) = 0.200 Root MSE = 11.38 

## a.
Parameters:

1. ${\beta}_0$ = 142.68 = The predicted muscle mass in a women when her X value, age is equal to 0
1. ${\beta}_1$ = -1.47 = The change in muscle mass for every one unit change in age

## b.
$\hat{Y}_{63} = 142.68 - 1.47*63$ = `r 142.68 - 1.47*63`

## c.
In order to calculate the correlation from the slope first multiply the slope of the regression line by the standard deviation of X and then divide by the standard deviation of Y.

Slope: -1.47

SDx: 14.67

SDy: 24.16

r = (Slope * SDx) / SDy = `r (-1.47 * 14.67) / 24.16`

## d.
State our hypotheses:

$H_0: r = 0$

$H_a: r \neq 0$

${\alpha}=0.05$

We would like to calculate the t value of the correlation. The t value's formula is: $t = \frac{r\sqrt(n-2)}{\sqrt{1-r^2}}$

t = `r (-0.8925869 * sqrt(14)) / sqrt(1 - -0.8925869* -0.8925869)`

p = `r 2*pt(-abs((-0.8925869 * sqrt(14)) / sqrt(1 - -0.8925869* -0.8925869)),df=14)`

Given this p value, we reject the null hypothesis and conclude there is a non-zero relationship between age and muscle mass.

## e. 
The 95% C.I. for ${\beta}_1$ = -1.47 +/- `r round(qt(p = .975, df=14) * .2, 4)`; [`r -1.47-round(qt(p = .975, df=14) * .2, 4)`,`r -1.47+round(qt(p = .975, df=14) * .2, 4)`]

I will interpret this later

## f. 
The 95% C.I. for a participant with an age of 63 is equal to [`r 142.68 - 1.899*63`,`r 142.68 - 1.041*63`]. 

I will interpret this later

## g.
The residual is equal to $Y - \hat{Y}$ = 45 - `r 142.68 - 1.47*60` = `r (142.68 - 1.47*60) - 45`

## h. 
PRE = `r -0.8925869 * -0.8925869`

${\beta}_{standardized} = {\beta}\frac{{SD}_x}{{SD}_y}$ = `r -1.47 * (14.67/24.16)`

Here we see that we explain roughly `r round(-0.8925869 * -0.8925869, 4)` percent of the total variance in the outcome. Furthermore, it appears that the that the standardized coefficient between age and muscle mass is: `r -1.47 * (14.67/24.16)` which suggests a very strong relationship between these two variables.

## i. 
If two women differ in age by 10 years the predicted difference in muscle mass will be -14.7 units.

## j.
It is difficult to tell if the linear assumption holds with these data. However, the provided model fits the data very well, thus the assumption that model provides some utility is tenable.

# Question 2
The following SAS output contains an analysis in which 6-year graduation rates were collected over a 6-year period and analyzed for trends.   The response variable Y is graduation rate (measured as a percentage). The explanatory variable X is year (beginning with year 1)

## a. 
$\hat{Y}=61.52667 + 0.95429*year$

## b. 
$\hat{Y}_6=61.52667 + 0.95429*6$ = `r 61.52667 + 0.95429*6`

## c.
The residual is equal to $Y - \hat{Y}$ = 67.8 - `r 61.52667 + 0.95429*5` = `r (61.52667 + 0.95429*5) - 67.8`

Here the residual is 1.5, meaning the value was underpredicted by 1.5%.

## d.
PRE = 0.7541 This PRE suggests that the additional variable yields a .7541 proportional reduction in error across this model and the null model. 

## e. 
The 95% C.I. for ${\beta}_1$ = 0.95429 +/- `r round(qt(p = .975, df=4) * 0.27247, 4)`; [`r 0.95429-round(qt(p = .975, df=4) * 0.27247, 4)`,`r 0.95429+round(qt(p = .975, df=4) * 0.27247, 4)`]

I will interpret this later

## f.
State our hypotheses:

$H_0: {\beta}_{year} = 0$

$H_a: {\beta}_{year} \neq 0$

${\alpha}=0.05$

We conclude that year is a significant predictor (t=3.50, p<0.05) and reject that null hypothesis that the relationship between year and graduation rate is not 0.

## g. 

Conditional standard deviation is equivalent to $\frac{1}{n}\Sigma_{n=1}^{i}(y-\hat{y})^2$ = MSE. Taking the root of the MSE gives you the conditional SD. Here the root mean squared error is equal to 1.13982, therefore the conditional SD is also 1.13982.

## h. 

This study had 6 observations, assuming all of the observations were unique, this study had 6 years worth of data.

## i.

The value 15.93 is the sum of squares for the model. Another phrase for this is the explained sum of squares (ESS). ESS reflects how much of the variance the model explains from the original total sum of squares.

## j. 

While the model reported here suggests a positive relationship between year and graduation rate, this does not restrict the data to only positive increases across years in graduation rates. As long as the model fit displays homoscedastic this is not a concern for the model that was fitted.

## k. 
${\beta}_{year}$ = 0.95429

$SD_x$ = `r sd(c(1,2,3,4,5,6))`

$SD_y$ = `r sqrt(21.13333*(1/5))`

${\beta}_{standardized} = {\beta}\frac{{SD}_x}{{SD}_y}$ = `r 0.95429 * (1.870829/2.065559)`

# Question 3
A criminologist studying the relationship between population density and robbery rates in medium-sized US cities collected the following data for a random sample of 16 cities; X is the population density of the city (number of people per unit area) and Y is the robbery rate last year (number of robberies per 100,000 people).  Assume that a simple linear regression model is appropriate.

I: 	1	2	3	4	5	6	7	8
X:	59	49	75	54	78	56	60	82
Y;	209	180	195	192	215	197	208	189
     
I:	9	10	11	12	13	14	15	16
X:	69	83	88	94	47	65	89	70
Y:	213	201	214	212	205	186	200	204


```{r q-3-data, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

## a. 
```{r q-3-a, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

## b. 
```{r q-3-b, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

## c.
```{r q-3-c, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

## d. 
Here the $\beta_{popdens}$ is equal to 0.2616 which suggests for every unit increase in population density, there is a 0.2616 unit increase in robbery rate.

## e.
The proportion reduction of error here is 0.1332, which suggests a small-medium effect between these two variables. This value tells us that we reduce this proportion in the error compared to a model that only includes an intercept term.

## f.
We want to calculate the confidence interval for the mean prediction. The associated formula is:

$\hat{Y}{\pm}t_{14}*\sqrt{MSE}*\sqrt{\frac{1}{n}\frac{(X_i-\overline{X})^2}{SS_x}}$

Values will be:

$\hat{Y}$ = `r predict(mod)[16]`

$t_{14}=$ `r qt(p = .975, df=14)`

$MSE=$ `r mse(mod)`

$n=$ 16

$X_i=$ 70

$\overline{X}=$ `r mean(q.three.dat$x)`

$SS_x=$ `r var(q.three.dat$x) * 15`

$C.I.M_{manual}$ = [`r round(predict(mod)[16] - (qt(p = .975, df=14)*sqrt(mse(mod)) * sqrt((1/16)+((70-mean(q.three.dat$x))^2/(var(q.three.dat$x) * 15)))),1)`, `r round(predict(mod)[16] + (qt(p = .975, df=14)*sqrt(mse(mod)) * sqrt((1/16)+((70-mean(q.three.dat$x))^2/(var(q.three.dat$x) * 15)))),1)`]


For safety, lets compare these estimates to the R predict function estimates:

$C.I.M_{rPredict}$ = [`r round(predict(mod, interval="confidence")[16,2], 1)`, `r round(predict(mod, interval="confidence")[16,3],1)`]

It appears we have generall consensus, although some diagreement at the tens decimal place...

## g. 
We want to calculate the confidence interval for an individual predicition. The associated formula is:

$\hat{Y}{\pm}t_{14}*\sqrt{MSE}*\sqrt{\frac{1}{n}\frac{(X_i-\overline{X})^2}{SS_x}}$

Values will be:

$\hat{Y}$ = `r predict(mod)[12]`

$t_{14}=$ `r qt(p = .975, df=14)`

$MSE=$ `r mse(mod)`

$n=$ 16

$X_i=$ 94

$\overline{X}=$ `r mean(q.three.dat$x)`

$SS_x=$ `r var(q.three.dat$x) * 15`

$C.I.M_{manual}$ = [`r round(predict(mod)[16] - (qt(p = .975, df=14)*sqrt(mse(mod)) * sqrt((1/16)+((94-mean(q.three.dat$x))^2/(var(q.three.dat$x) * 15)))),4)`, `r round(predict(mod)[16] + (qt(p = .975, df=14)*sqrt(mse(mod)) * sqrt((1/16)+((94-mean(q.three.dat$x))^2/(var(q.three.dat$x) * 15)))),4)`]

For safety, lets compare these estimates to the R predict function estimates:

$C.I.M_{rPredict}$ = [`r predict(mod, interval="confidence")[12,2]`, `r predict(mod, interval="confidence")[12,3]`]

It appears we have no consensus, although some diagreement at the tens decimal place...

## h.
Here we want to estimate the 95% confidence interval for the slope of the regression model.

In order to perform this we are going to use the following formula:

$\hat\beta{_1} \pm t_{\alpha/2;df=n-2}*se(\hat\beta_{1})$

With the standard error of $\hat\beta_1$ calculated as:

se = $\sqrt{\frac{MSE}{\Sigma^n_{i=1}(x_i-\overline{x})^2}}$

Values will be:

$\beta_{1} =$ 0.2616

$t_{14}=$ `r qt(p = .975, df=14)`

$MSE = $ `r mse(mod)`

$SS_x=$ `r var(q.three.dat$x) * 15`

Accordingly, the 95% CI for $\beta_1$ is equal to [`r round(confint(mod)[2,1], 2)`, `r round(confint(mod)[2,2], 2)`]

## i.
```{r q-3-i, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

As we observe from the preceding plots, these data appear to display homoscedastic errors, they are independent samples, and they have also been randomly sampled. These characteristics taken together means the linear regression model fitted here is appropriate for the data.

# Question 4
Analysis of Variance
Model DF: 1

Model SS: 9.874148

Model MSE: 9.874148

Model F: 53.27865

Error DF: 48

Error SS: 8.895852

Error MSE: 0.1853303

Corrected Total DF: 49

R-square: 0.5260601

Parameter Estimates

intercept standard error: 0.1793638

intercept standardized estimate: 0

size parameter estimate: 0.414932

Pr > |t|: `r pt(7.3, 48, lower.tail = F)`

size standardized estimate: 0.7253