---
title: "Final"
author: "Adon Rosen"
date: 'Date: `r Sys.Date()`'
output: pdf_document
---
  
```{r, echo=F, message=F, warning=FALSE}
library(knitr)
opts_knit$set(root.dir='./')  #Don't combine this call with any other chunk -especially one that uses file paths.
```


<!-- Set the report-wide options, and point to the external code file. -->
```{r set_options, echo=F, warning=FALSE}
# cat("Working directory: ", getwd())
report_render_start_time <- Sys.time()
opts_chunk$set(
  results    = 'show',
  comment    = NA,
  tidy       = FALSE,
  fig.width  = 10,
  fig.height = 6,
  fig.path   = 'figure-png/'
)
# echoChunks <- FALSE
options(width=80) #So the output is 50% wider than the default.
read_chunk("./Final.R") #This allows knitr to call chunks tagged in the underlying *.R file.
```


```{r load-packages, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```
```{r load-data, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```
```{r declare-functions, echo=FALSE, results='show', message=FALSE, warning=FALSE}
```

# Question 1

You are interviewing for a job and you will be asked to teach a course in linear models.  To ascertain your competence and vision for the course, you are asked to give the 10 most important ideas, principles, or practical matters you have learned in this course.  To help, make sure to not simply list 10 things - be sure to justify your ranking of the 10 most important things ... why you chose the ones you did and why did you rank them as you did? At least 2000 words for a sufficient answer

## 1. Independence of observations

## 2. Multivariate Normality

## 3. Homoskedastic error distribution

## 4. Model building techniques

## 5. Multicollinearity

## 6. Model comparison

Like any good statistical test, the inferences that can be made are limited by the experiment being performed. This is why the highest priority component when describing linear regression models includes the assumptions of linear models which are points one through four listed above, beginning with the independence of the observations. This point is made in most parametric statistical examinations, for instance the one group t-test assumes a random sample, normally distributed data, and equality of variance. Hidden within the assumption of random sampling is the importance of independence amongst the observations. In respect to linear regression, independence amongst the observations protects against autocorrelation being introduced into the predictions. Autocorrelation is a phenomenon where residual values display relationships with themselves. When an autocorrelation structure is present, this reduces standard error estimates inflating estimated significance of parameters, which is something that can be easily protected against by employing simple random sampling. The next listed topic is that of multivariate normality, another issue that when dealt with improperly can reduce the ability to make statistically accurate tests.

    Multivariate assumptions are usually tested visually using a qq-plot. A qq-plot displays the relationship between the standardized residuals and the theoretical quantiles. THe theoretical quantiles are taken from a z-distribution allowing for quantiles to be obtained from a theoretically normal distribution. When multivariate normality is met, these distributions should be near identical and follow an identity line (x=y). When multivariate normality assumptions are met, it ensures that the statistical power of the underlying linear model is met. In contrast, when multivariate normality is not met it reduces the power of the test, explicitly, it reduces the ability to find an effect when one is present. Such patterns have received extensive exploration as they are detrimental when these effects are understudied for instance Hopkins and Clay explored how variance in the samples' multivariate relationship effect inferences being made, suggesting alterations in type I and type II error rates (1963). This research has continued to be of significant interest to researchers and similar studies are still being performed well into the information era. This proliferation of methods to analyze multivariate normality, and explore the impacts of these assumptions underscores the importance this carries into linear regression. Building off of these explorations, analysis of multivariate normality allows researchers to explore the underlying distribution of effects which can vary in shape.

    Linear models can take many forms, the general assumption is that the relationship between the independent and dependent variables follows a linear trend. That is the relationship between the variables can be modeled using a straight line. However when deviations from these assumptions occur it alters the previous assumption of multivariate normality thus hindering the ability for inferences to be made statistically. This concern can be introduced in anticipated manners such as when the dependent variable is not normally distributed such as for binary outcomes, count outcomes, or highly skewed rare event prediction. However these trends can also occur when data takes an unanticipated form such as nonlinear functions including quadratic and cubic shapes. For the former, these relationships can be modeled using a variety of link functions that work by imposing a distribution on  the error term . Take for instance a binary outcome using a logit link, these transform the data, through exponentiation, such that the outcome of the model are now propabilities constrained between 0 and 1. Other such links exist, for instance a poisson distribution works for count occurances (i.e. when data take the form of frequencies of 0, 1, 2, or more events), a gamma function can be used to model exponential response data (i.e. a dependent variable that displays extreme skew). For the latter case, when data follow a quadratic or cubic trend, transformations can be made to the predictors, such as scaling and then squaring or cubing the predictors and then modeling these relationships again with an identity link function. These issues are usually flagged following the same methodology as for the multivariate assumption: using a qq plot, a final issue that needs to be satisfied for linear models which can further be analyzed by examining trends in the residuals is homoscedasticity.

    Homoscedasticity exists in multivariate relationships when error terms display equal variance across the entire distribution. This point, while distinct from independence of observations, builds off of the same logic. When error terms do display heteroscedasticity, it means that variance-covariance relationships are not consistent across the entire random distribution. Put in another way this occurs when sizes of the residuals change as the values of the predicted outcomes changes. This is typically due to some potential autocorrelation introduced in the data. An example of these which potentially distinguishes this from independence of the observations is when multiple dependent observations are being studied from individuals within the breath of one study. THis maintains independence of observations across the population, but introduces a plausible intraclass correlation among the repeated measures. This, however, does create a similar effect as briefly described, when data are not homoscedastic, standard error estimates are likely to be reduced, again, this will inflate the absolute statistical estimate inflating the relative importance of predictors when standard errors are reduced. It is however, interesting to note, this does not affect the estimation of the coefficient, it impacts the statistical weight given to a coefficient (i.e. the p-value). This can be troublesome when inferential statistics are sought after, or when attempting to build the most parsimonious model.

    Inferential statistics when used in concert with linear modeling, allows for researchers to distinguish the likelihood of a coefficient obtaining the magnitude of the observed effect when the null hypothesis is true. The canonical null hypothesis for testing coefficients is to state that the relationship is 0. This logic can be used to trim down models, and is employed in techniques such as stepwise model building. Stepwise modeling is performed by finding the relationship between a set of predictors, and a dependent variable with the strongest absolute magnitude. It is an automated procedure which allows for models to be built with little to no input from the researchers. Model building has become an ever growing demand as statistics transition from inferential to predictive modeling. Predictive modeling involves creating a model which predicts the outcome to the best degree; whereas, inferential seeks to find significant relationships between the independent and dependent variables. The goal of model building is to find the absolute best fitting model given the current data. This goal has long alluded practitioners of statistics given the fact that a linear model, in its raw form, can not lose its predictive nature when non influential variables are introduced. However, these nuisance variables bias the predictive nature of the model when introduced to unseen data. Which is something stepwise regression model building seeks to perform in a relatively unguided manner. Inputs to stepwise regression algorithms typically include input and output p-values which guide inclusion and exclusion of predictor variables. Newer advances to model building, driven predominantly by the romantics era and the expansion of big data, have grown to include more automatic  one step algorithms for variable selection.


# Question 2

## a.

The numerator degrees of freedom is: 19

The denonminator degrees of freedom is: 3

THe numerator degres of freedom reflects the values consumed when training the model, that is these are the values that are estimated from the sample to be popluation level effects. THe denominator degrees of freedom reflect the elements that remain free in the data after traiing the model

## b. 

The n for this model was: 23

## c. 

The root MSE is: 10.28945

This value reflects the conditional standard deviation, or the standard deviation of the residual values.

## d.

The dependent variables mean value is: 61.34783

## e.

The best estimate for the PRE is 0.6210; this is the PRE when taking in to account the number of variables used to train the model.

## f.

The value of the intercept term is 162.87590

This value reflects the predicted outcome when all independent variables are 0.

## g.

The variable with the greatest predictive influence when accounting for covariates is x3

This can be observed in various ways, perhaps the most convincing is the t value, which is the largest t value for any predictor other than the intercept, furthermore the standardaized coefficeint is also the largest, suggesting the strongest conditioned relationship exists between the DV and x3.

## h.

A parameter estimate reflects the change in the DV in one unit change of the IV.

For instance, using the raw coefficients, as x3 changes from 3 to 4; there is a decrease in y of -1.21032 units.

## i. 

A semi-partial correlation is the relationship between two variables when variance from one or more variables is removed from one of the variables of interest. WHen this relationship is squared it gives the coefficient of determination after controlling fro any shared variance from one of the predictors.

A partial correlation describes the realtionship between 

# Question 3

Model DF: 3

Model Sum of Squares: `r (9.1226852 * qf(0.9998, df1=3, df2=18)) * 3`

Model Mean Square: `r 9.1226852 * qf(0.9998, df1=3, df2=18)`

Model F value: `r qf(0.9998, df1=3, df2=18)`

Error DF: 18

Error Sum of Squares: `r 489.8181818 - (9.1226852 * qf(0.9998, df1=3, df2=18))`

Corrected Total DF: 21

Root MSE: `r sqrt(9.1226852 * qf(0.9998, df1=3, df2=18))`

### Type 1 Anova Table

Prof DF: 1

Prof Type I SS: `r 27.65 * 9.1226852`

Prof Mean Square: `r 27.65 * 9.1226852`

Sex DF: 1

Sex Type I SS: `r 7.98 * 9.1226852`

Sex Mean Square: `r 7.98 * 9.1226852`

PROF$*$SEX DF: 1

PROF$*$SEX Type I SS: `r qf(.2049, df1=1, df2=18) * 9.1226852`

PROF$*$SEX Mean Square: `r qf(.2049, df1=1, df2=18) * 9.1226852`

PROF$*$SEX F Value: `r qf(.2049, df1=1, df2=18)`

### Type II Anova Table

PROF DF: 1

SEX DF: 1

PROF$*$SEX DF: 1

PROF$*$SEX Type II SS: 0.6337719

PROF$*$SEX F Value: `r qf(.2049, df1=1, df2=18)`

### Type III Anova Table

PROF DF: 1

SEX DF: 1

PROF$*$SEX DF: 1

PROF$*$SEX Type III SS: `r qf(.2049, df1=1, df2=18) * 9.1226852`

PROF$*$SEX Mean Square: `r qf(.2049, df1=1, df2=18) * 9.1226852`

PROF$*$SEX F Value: `r qf(.2049, df1=1, df2=18)`